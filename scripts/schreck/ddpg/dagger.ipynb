{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import traceback\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import yaml\n",
    "import copy\n",
    "import time \n",
    "import os\n",
    "\n",
    "from ddpg.reader import LoadGeckoPandasExperiment\n",
    "from ddpg.replay_buffer import ReplayBuffer\n",
    "from ddpg.trainer import Trainer\n",
    "#from ddpg.checkpoint import *\n",
    "from ddpg.losses import GeckoLoss\n",
    "from ddpg.ddpg import DDPG\n",
    "\n",
    "from holodecml.vae.optimizers import *\n",
    "from holodecml.vae.tqdm import tqdm\n",
    "from holodecml.vae.checkpointer import *\n",
    "\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/dagger/config.yml\") as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data = LoadGeckoPandasExperiment(**config[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../clustered/experiment_data/experiment_train_test_val_splits.pkl\"\n",
    "with open(fn, \"rb\") as fid:\n",
    "    train, valid, test = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = LoadGeckoPandasExperiment(\n",
    "    **config[\"data\"],\n",
    "    experiment_subset = train,\n",
    "    x_data = experiment_data.x,\n",
    "    y_data = experiment_data.y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"data\"][\"shuffle\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_set = LoadGeckoPandasExperiment(\n",
    "    **config[\"data\"],\n",
    "    experiment_subset = valid,\n",
    "    x_data = experiment_data.x,\n",
    "    y_data = experiment_data.y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = LoadGeckoPandasExperiment(\n",
    "    **config[\"data\"],\n",
    "    experiment_subset = test,\n",
    "    x_data = experiment_data.x,\n",
    "    y_data = experiment_data.y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to use device cpu\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "print(f'Preparing to use device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 output_size, \n",
    "                 hidden_dims = [100, 50], \n",
    "                 dropouts = [0.2, 0.2]):\n",
    "        \n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        #self.embedding = nn.Embedding(train_data_set.num_timesteps, 16)\n",
    "        \n",
    "        self.model = [\n",
    "            nn.Linear(input_size, hidden_dims[0]),\n",
    "            #nn.BatchNorm1d(num_features=hidden_dims[0]),\n",
    "            nn.LeakyReLU() # nn.SELU()\n",
    "        ]\n",
    "        if len(hidden_dims) > 1:\n",
    "            if dropouts[0] > 0.0:\n",
    "                self.model.append(nn.Dropout(dropouts[0]))\n",
    "            for i in range(len(hidden_dims)-1):\n",
    "                self.model.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                #self.model.append(nn.BatchNorm1d(num_features=hidden_dims[i+1]))\n",
    "                self.model.append(nn.LeakyReLU())\n",
    "                if dropouts[i+1] > 0.0:\n",
    "                    self.model.append(nn.Dropout(dropouts[i+1]))\n",
    "        self.model.append(nn.Linear(hidden_dims[-1], output_size))\n",
    "        self.model.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x1, x2 = x\n",
    "        #x1 = self.embedding(x1)\n",
    "        #x = torch.cat([x1, x2], 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DenseNet(**config[\"model\"])\n",
    "expert = DenseNet(**config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid = torch.load(\"results/pretrain/best.pt\", map_location=lambda storage, loc: storage)\n",
    "\n",
    "#policy.load_state_dict(fid[\"model_state_dict\"])\n",
    "expert.load_state_dict(fid[\"model_state_dict\"])\n",
    "\n",
    "# critic_dict = torch.load(\"results/100/critic_best.pt\", map_location=lambda storage, loc: storage)\n",
    "# policy.critic.load_state_dict(critic_dict[\"model_state_dict\"])\n",
    "# policy.critic_target = copy.deepcopy(policy.critic)\n",
    "\n",
    "# actor_dict = torch.load(\"results/100/actor_best.pt\", map_location=lambda storage, loc: storage)\n",
    "# policy.actor.load_state_dict(actor_dict[\"model_state_dict\"])\n",
    "# policy.actor_target = copy.deepcopy(policy.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, device, min_size=int(1e4), max_size=int(1e6)):\n",
    "        self.min_size = int(min_size)\n",
    "        self.max_size = int(max_size)\n",
    "        self.ptr = 0\n",
    "        self.size = 0 \n",
    "\n",
    "        self.input = np.zeros((self.max_size, state_dim))\n",
    "        self.output = np.zeros((self.max_size, state_dim))    \n",
    "        self.device = device\n",
    "\n",
    "    def add(self, x, y):\n",
    "        self.input[self.ptr] = x\n",
    "        self.output[self.ptr] = y\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            torch.FloatTensor(self.input[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.output[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(**config[\"replay_buffer\"], device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LookaheadDiffGrad(policy.parameters(),\n",
    "                              lr=config[\"optimizer\"][\"lr\"])\n",
    "#                              weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, \n",
    "                 policy, \n",
    "                 expert,\n",
    "                 optimizer,\n",
    "                 train_generator, \n",
    "                 validation_generator, \n",
    "                 test_generator, \n",
    "                 replay_buffer,\n",
    "                 device,\n",
    "                 start_epoch = 0,\n",
    "                 epochs = 100,\n",
    "                 batches_per_epoch = 1e10,\n",
    "                 experiment_batch_size = 1,\n",
    "                 batch_size = 64,\n",
    "                 max_timestep = 1439,\n",
    "                 teacher_force = False, \n",
    "                 gamma = 1.0,\n",
    "                 clip = 1.0,\n",
    "                 max_state = 1.0,\n",
    "                 expl_noise = 0.1):\n",
    "        \n",
    "        self.start_epoch = start_epoch\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.batches_per_epoch = batches_per_epoch\n",
    "        self.experiment_batch_size = experiment_batch_size\n",
    "        \n",
    "        self.train_gen = train_generator\n",
    "        self.valid_gen = validation_generator\n",
    "        self.test_gen = test_generator\n",
    "        \n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.max_timestep = max_timestep\n",
    "        \n",
    "        self.policy = policy\n",
    "        self.expert = expert\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.teacher_force = teacher_force\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.device = device\n",
    "        self.clip = clip\n",
    "        self.max_state = max_state\n",
    "        self.expl_noise = expl_noise \n",
    "        \n",
    "        self.beta = np.linspace(1.0, 0.0, self.epochs, endpoint = True)\n",
    "                \n",
    "        #############################################\n",
    "        #\n",
    "        # Gradient clipping through hook registration\n",
    "        #\n",
    "        #############################################\n",
    "        for p in self.policy.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -clip, clip))\n",
    "        logger.info(f\"Clipping gradients to range [-{clip}, {clip}]\")\n",
    "        \n",
    "        #############################################\n",
    "        #\n",
    "        # Set expert policy to eval mode\n",
    "        #\n",
    "        #############################################\n",
    "        self.expert.eval()\n",
    "        \n",
    "        #############################################\n",
    "        #\n",
    "        # Fix the number of batches per epoch\n",
    "        #\n",
    "        #############################################\n",
    "        batches_per_epoch = int(self.train_gen.__len__()) \n",
    "        if self.batches_per_epoch > batches_per_epoch:\n",
    "            self.batches_per_epoch = batches_per_epoch\n",
    "        \n",
    "        #############################################\n",
    "        #\n",
    "        # Load the training experiments and randomly shuffle\n",
    "        #\n",
    "        #############################################\n",
    "        self.train_experiments = list(range(len(self.train_gen.experiment_subset)))\n",
    "        self.train_experiments_copy = list(range(len(self.train_gen.experiment_subset)))\n",
    "        random.shuffle(self.train_experiments)\n",
    "        if self.experiment_batch_size > 1:\n",
    "            self.train_experiments = list(chunks(\n",
    "                self.train_experiments, \n",
    "                self.experiment_batch_size\n",
    "            ))\n",
    "        self.reshuffle = 0 # when == len(self.train_experiments), reshuffle\n",
    "    \n",
    "    def train_one_epoch(self, epoch):\n",
    "        \n",
    "        self.policy.train()\n",
    "        \n",
    "        if self.reshuffle == len(self.train_experiments):\n",
    "            random.shuffle(self.train_experiments)\n",
    "            if self.experiment_batch_size > 1:\n",
    "                self.train_experiments = list(chunks(\n",
    "                    self.train_experiments_copy, \n",
    "                    self.experiment_batch_size\n",
    "                ))\n",
    "            self.reshuffle = 0\n",
    "        \n",
    "        batch_group_generator = tqdm(\n",
    "            enumerate(self.train_experiments[self.reshuffle:]),\n",
    "            total=min(self.batches_per_epoch,len(self.train_experiments[self.reshuffle:])),\n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "        cost = self.tf_annealer(epoch)\n",
    "        \n",
    "        total_loss = []\n",
    "        for batch_idx, exp in batch_group_generator:\n",
    "            for i, (x,y,w) in enumerate(self.train_gen.__getitem__(exp)):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                w = w.to(self.device)\n",
    "\n",
    "                if i == 0:\n",
    "                    x_pred = x.clone()\n",
    "                else:\n",
    "                    x_pred[:, :29] = y_pred \n",
    "                    \n",
    "                y_pred = self.policy(x_pred)\n",
    "                y_expert = self.expert(x_pred if epoch > 0 else x) #assumes we start with expert as policy\n",
    "                        \n",
    "                for j in range(x.size(0)):\n",
    "                    self.replay_buffer.add(\n",
    "                        y_pred[j].cpu().detach().numpy(), \n",
    "                        y_expert[j].cpu().detach().numpy()\n",
    "                    )\n",
    "                    \n",
    "                # Evaluate also on the true input x. \n",
    "                _y_pred = self.policy(x)\n",
    "                _y_expert = self.expert(x)\n",
    "                    \n",
    "                for j in range(x.size(0)):\n",
    "                    self.replay_buffer.add(\n",
    "                        _y_pred[j].cpu().detach().numpy(), \n",
    "                        _y_expert[j].cpu().detach().numpy()\n",
    "                    )\n",
    "        \n",
    "            # Train on memory buffer subset of D u D(i)\n",
    "            if (self.replay_buffer.size >= self.replay_buffer.min_size):\n",
    "                \n",
    "                y_predict, y_expert = self.replay_buffer.sample(self.batch_size)\n",
    "                \n",
    "                # Compute DAgger loss\n",
    "                loss = Variable(\n",
    "                    F.mse_loss(y_predict, y_expert).type(torch.float32), requires_grad=True\n",
    "                ).to(self.device)\n",
    "                total_loss.append(loss.item())\n",
    "                \n",
    "                # Optimize the learner policy\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                for curr_param, expert_param in zip(self.policy.parameters(), self.expert.parameters()):\n",
    "                    curr_param.data.copy_(\n",
    "                        self.beta[epoch] * expert_param.data + (1 - self.beta[epoch]) * curr_param.data\n",
    "                    )\n",
    "                                 \n",
    "            # update tqdm\n",
    "            ave_loss = np.mean(total_loss) if len(total_loss) > 0 else 0.0\n",
    "            to_print = f\"Epoch {epoch + 1} training loss: {ave_loss:.3f}\"\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "            \n",
    "            self.reshuffle += 1\n",
    "            if batch_idx > 0 and (batch_idx % self.batches_per_epoch) == 0:\n",
    "                break\n",
    "            \n",
    "        return np.mean(total_loss)\n",
    "    \n",
    "        \n",
    "    def test(self, epoch):\n",
    "        self.policy.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_group_generator = tqdm(\n",
    "                enumerate(range(len(self.valid_gen.experiment_subset))),\n",
    "                total=len(self.valid_gen.experiment_subset), \n",
    "                leave=True\n",
    "            )\n",
    "            total_loss = []\n",
    "            for batch_idx, exp in batch_group_generator:\n",
    "                for i, (x,y,w) in enumerate(self.valid_gen.__getitem__(exp)):\n",
    "                    x = x.to(self.device)\n",
    "                    y = y.to(self.device)\n",
    "        \n",
    "                    if i == 0:\n",
    "                        x_pred = x.clone()\n",
    "                    else:\n",
    "                        x_pred[:, :29] = y_pred\n",
    "                        \n",
    "                    y_pred = self.policy(x_pred)\n",
    "                    total_loss.append(F.mse_loss(y_pred, y).item())\n",
    "                    \n",
    "                # update tqdm\n",
    "                ave_loss = np.mean(total_loss)\n",
    "                to_print = f\"Epoch {epoch + 1} validation loss: {ave_loss:.3f}\"\n",
    "                batch_group_generator.set_description(to_print)\n",
    "                batch_group_generator.update()\n",
    "\n",
    "        return np.mean(total_loss)\n",
    "    \n",
    "    def train(self,\n",
    "              scheduler,\n",
    "              early_stopping,\n",
    "              metrics_logger):\n",
    "            \n",
    "        logger.info(\n",
    "            f\"Training the model for up to {self.epochs} epochs starting at epoch {self.start_epoch}\"\n",
    "        )\n",
    "\n",
    "        flag = isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "        \n",
    "        for episode in range(self.start_epoch, self.epochs):\n",
    "            train_loss = self.train_one_epoch(episode)\n",
    "            test_loss = self.test(episode)\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(test_loss if flag else episode)\n",
    "            \n",
    "            early_stopping(\n",
    "                episode, \n",
    "                test_loss, # we want to maximize the reward rather than min a loss\n",
    "                self.policy, \n",
    "                self.optimizer\n",
    "            )\n",
    "\n",
    "            # Write results to the callback logger \n",
    "            result = {\n",
    "                \"episode\": episode,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": test_loss,\n",
    "                \"lr\": early_stopping.print_learning_rate(self.optimizer),\n",
    "                \"forcing_score\": self.tf_annealer(episode) if self.teacher_force else 1.0\n",
    "            }\n",
    "            metrics_logger.update(result)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping\")\n",
    "                break\n",
    "                \n",
    "    def tf_annealer(self, epoch):\n",
    "        return 1.0 * self.gamma ** epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    policy, \n",
    "    expert,\n",
    "    optimizer,\n",
    "    train_data_set, \n",
    "    valid_data_set, \n",
    "    test_data_set, \n",
    "    replay_buffer,\n",
    "    device = device,\n",
    "    **config[\"trainer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "if \"ReduceLROnPlateau\" in config[\"callbacks\"]:\n",
    "    schedule_config = config[\"callbacks\"][\"ReduceLROnPlateau\"]\n",
    "    scheduler = ReduceLROnPlateau(trainer.optimizer, **schedule_config)\n",
    "    #logging.info(\n",
    "    #    f\"Loaded ReduceLROnPlateau learning rate annealer with patience {schedule_config['patience']}\"\n",
    "    #)\n",
    "elif \"ExponentialLR\" in config[\"callbacks\"]:\n",
    "    schedule_config = config[\"callbacks\"][\"ExponentialLR\"]\n",
    "    scheduler = ExponentialLR(trainer.optimizer, **schedule_config)\n",
    "    #logging.info(\n",
    "    #    f\"Loaded ExponentialLR learning rate annealer with reduce factor {schedule_config['gamma']}\"\n",
    "    #)\n",
    "\n",
    "# Early stopping\n",
    "checkpoint_config = config[\"callbacks\"][\"EarlyStopping\"]\n",
    "early_stopping = EarlyStopping(**checkpoint_config)\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(**config[\"callbacks\"][\"MetricsLogger\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 0.000: 100%|██████████| 320/320 [11:52<00:00,  2.23s/it]\n",
      "Epoch 1 validation loss: 0.083: 100%|██████████| 201/201 [02:04<00:00,  1.61it/s]\n",
      "Epoch 2 training loss: 0.061: 100%|██████████| 320/320 [11:35<00:00,  2.17s/it]\n",
      "Epoch 2 validation loss: 0.120: 100%|██████████| 201/201 [01:52<00:00,  1.78it/s]\n",
      "Epoch 3 training loss: 0.037: 100%|██████████| 320/320 [11:24<00:00,  2.14s/it]\n",
      "Epoch 3 validation loss: 0.120: 100%|██████████| 201/201 [02:05<00:00,  1.60it/s]\n",
      "Epoch 4 training loss: 0.026: 100%|██████████| 320/320 [11:14<00:00,  2.11s/it]\n",
      "Epoch 4 validation loss: 0.120: 100%|██████████| 201/201 [01:48<00:00,  1.84it/s]\n",
      "Epoch 5 training loss: 0.020: 100%|██████████| 316/316 [11:12<00:00,  2.13s/it]\n",
      "Epoch 5 validation loss: 0.120: 100%|██████████| 201/201 [01:54<00:00,  1.76it/s]\n",
      "Epoch 6 training loss: 0.017: 100%|██████████| 320/320 [10:48<00:00,  2.03s/it]\n",
      "Epoch 6 validation loss: 0.120: 100%|██████████| 201/201 [01:51<00:00,  1.80it/s]\n",
      "Epoch 7 training loss: 0.014: 100%|██████████| 320/320 [10:19<00:00,  1.94s/it]\n",
      "Epoch 7 validation loss: 0.118:  35%|███▍      | 70/201 [00:39<00:55,  2.36it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(scheduler, early_stopping, metrics_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
