{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from os.path import join, exists\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from holodecml.vae.checkpointer import *\n",
    "from holodecml.vae.optimizers import *\n",
    "from holodecml.vae.tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import *\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to use device cuda:0\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(torch.cuda.current_device()) if is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "if is_cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f'Preparing to use device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/9_23/pandas/config_experiment.yml\") as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadGeckoData:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 save_path,\n",
    "                 summary_file,  \n",
    "                 bin_prefix = [], \n",
    "                 input_vars = [],\n",
    "                 output_vars = [],\n",
    "                 seq_length = 1, \n",
    "                 num_timesteps = 1439,\n",
    "                 experiment_subset = [],\n",
    "                 cached_dir = \"./\",\n",
    "                 shuffle = True,\n",
    "                 scaler_x = None,\n",
    "                 scaler_y = None, \n",
    "                 fit = False, \n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        self.path = data_path\n",
    "        self.save_path = save_path\n",
    "        self.summary_file = summary_file\n",
    "        self.experiment_subset = experiment_subset\n",
    "        self.cached_dir = cached_dir\n",
    "        self.bin_prefix = bin_prefix\n",
    "        self.input_vars = input_vars\n",
    "        self.output_vars = output_vars\n",
    "        self.seq_length = 1\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.scaler_x = scaler_x\n",
    "        self.scaler_y = scaler_y\n",
    "        self.fit = fit\n",
    "        \n",
    "        self.load()\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.reshape = True\n",
    "        if any([self.scaler_x is None, self.scaler_y is None]) or self.fit:\n",
    "            self.fit_scalers()\n",
    "            \n",
    "    def load(self):\n",
    "        file_list = glob.glob(os.path.join(self.path, 'ML2019_*'))\n",
    "        self.file_list = sorted(file_list, key = lambda x: int(x.split(\"Exp\")[1].strip(\".csv\")))\n",
    "        self.summary_file = pd.read_csv(\n",
    "            os.path.join(self.path, self.summary_file), skiprows = 3\n",
    "        )\n",
    "        self.summary_file.columns = [x.strip() for x in self.summary_file.columns]\n",
    "    \n",
    "    def get_transform(self):\n",
    "        return self.scaler_x, self.scaler_y\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return len(self.experiment_subset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        'Generate one data point'\n",
    "        \n",
    "        ### Find the relevant experiment file \n",
    "        exp = self.experiment_subset[idx]\n",
    "        \n",
    "        ### If we have this experiment already cached, load it\n",
    "        cached = f\"{self.cached_dir}/{exp}.pkl\"\n",
    "        if os.path.isfile(cached) and self.scaler_x is not None:\n",
    "            with open(cached, \"rb\") as fid:\n",
    "                return pickle.load(fid)\n",
    "        \n",
    "        ### Else we have to create it for the first time.\n",
    "        for file_path in self.file_list:\n",
    "            if f\"Exp{exp}\" in file_path:\n",
    "                break\n",
    "\n",
    "        ### Load the experiment file\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.columns = [x.strip() for x in df.columns]\n",
    "\n",
    "        ### Load the summary file\n",
    "        exp_num = int(re.findall(\"_Exp(\\d+)*\", file_path)[0])\n",
    "        summary_file = self.summary_file[(\n",
    "            self.summary_file['id'] == f\"Exp{exp_num}\"\n",
    "        )].copy()\n",
    "\n",
    "        for variable in summary_file.columns:\n",
    "            df[variable] = summary_file[variable][exp_num]\n",
    "\n",
    "        if len(self.bin_prefix) > 0: \n",
    "            for prefix in self.bin_prefix:\n",
    "                df[prefix] = df.loc[:, df.columns.str.contains(prefix, regex=False)].sum(axis=1)\n",
    "\n",
    "        df = self.add_diurnal_signal(df)\n",
    "                \n",
    "        input_subset = df[self.input_vars].iloc[:-1,:].copy()\n",
    "        output_subset = df[self.output_vars].iloc[1:,:].copy()\n",
    "\n",
    "        if \"index\" in input_subset.columns:\n",
    "            input_subset = input_subset.drop(columns = [\"index\"])\n",
    "        if \"index\" in output_subset.columns:\n",
    "            output_subset = output_subset.drop(columns = [\"index\"])\n",
    "\n",
    "        #input_subset = self.add_diurnal_signal(input_subset)\n",
    "        #output_subset = self.add_diurnal_signal(output_subset)\n",
    "\n",
    "        self.processed += 1\n",
    "        if self.processed == self.__len__():\n",
    "            self.on_epoch_end()\n",
    "\n",
    "        if self.scaler_x is not None:\n",
    "            input_subset = self.scaler_x.transform(\n",
    "                input_subset.drop(['Time [s]', 'id'], axis=1)\n",
    "            )\n",
    "        if self.scaler_y is not None:\n",
    "            output_subset = self.scaler_y.transform(\n",
    "                output_subset.drop(['Time [s]', 'id'], axis=1)\n",
    "            )\n",
    "        if self.reshape:\n",
    "            input_subset, output_subset = self.reshape_data(input_subset, output_subset)\n",
    "            \n",
    "            with open(cached, \"wb\") as fid:\n",
    "                pickle.dump([input_subset, output_subset], fid)\n",
    "\n",
    "        return input_subset, output_subset\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.processed = 0\n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(self.file_list)\n",
    "            \n",
    "    def add_diurnal_signal(self, x_data):\n",
    "        \"\"\"\n",
    "        Apply Function to static temperature to add +- 4 [K] diurnal signal (dependent of time [s] of timeseries).\n",
    "        Args:\n",
    "            x_data: Pre-scaled/normalized input data (Pandas DF).\n",
    "\n",
    "        Returns: Same df with function applied to temperature feature.\n",
    "        \"\"\"\n",
    "        x_data['temperature (K)'] = x_data['temperature (K)'] + 4.0 * np.sin(\n",
    "            (x_data['Time [s]'] * 7.2722e-5 + (np.pi / 2.0 - 7.2722e-5 * 64800.0)))\n",
    "\n",
    "        return x_data\n",
    "    \n",
    "    def get_tendencies(self, df):\n",
    "        \"\"\"\n",
    "         Transform dataframe to time tendencies rather than actual values. Preserves static environmental values.\n",
    "        Args:\n",
    "            df: Pre-scaled input dataframe.\n",
    "            input_cols: Input columns to be transformed (should include 'id' and 'Time' for indexing).\n",
    "        Returns: Pandas dataframe with input columns transformed to tendencies (Removes the first sample of each Exp).\n",
    "        \"\"\"\n",
    "        df_copy = df.copy()\n",
    "        dummy_df = df[self.output_vars].drop(['Time [s]'], axis=1).groupby('id').diff().reset_index(drop=True)\n",
    "        df_copy[self.output_vars[1:-1]] = dummy_df.values\n",
    "        df_copy.loc[:, ~df_copy.columns.isin(self.output_vars)] = df.loc[:, ~df.columns.isin(self.output_vars)]\n",
    "        dff = df_copy.groupby('id').apply(lambda x: x.iloc[1:, :]).reset_index(drop=True)\n",
    "        return dff\n",
    "    \n",
    "    def fit_scalers(self):\n",
    "\n",
    "        filepath = os.path.join(self.save_path, \"scalers.pkl\")\n",
    "        \n",
    "        if os.path.isfile(filepath) and not self.fit:\n",
    "            #logging.info(f\"Loading data preprocessing models from {filepath}\")\n",
    "            with open(filepath, \"rb\") as fid:\n",
    "                self.num_timesteps, self.scaler_x, self.scaler_y = pickle.load(fid)\n",
    "            \n",
    "        else:\n",
    "            #logging.info(\"Fitting data preprocessing models: QuantileTransformer\")\n",
    "            self.reshape = False                    \n",
    "            with Pool(24) as p:\n",
    "                xs, ys = zip(*[(x,y) for (x,y) in tqdm(\n",
    "                    p.imap(self.__getitem__, range(len(self.experiment_subset))),\n",
    "                    total = len(self.experiment_subset))\n",
    "                ])\n",
    "            p.join()\n",
    "            p.close()                    \n",
    "                    \n",
    "            xs = pd.concat(xs)\n",
    "            ys = pd.concat(ys)\n",
    "            self.reshape = True\n",
    "            \n",
    "            self.num_timesteps = xs['Time [s]'].nunique()\n",
    "\n",
    "            self.scaler_x = Pipeline(\n",
    "                steps=[('quant', QuantileTransformer()), ('minmax', MinMaxScaler((0, 1)))]\n",
    "            )\n",
    "            self.scaler_y = Pipeline(\n",
    "                steps=[('quant', QuantileTransformer()), ('minmax', MinMaxScaler((0, 1)))]\n",
    "            )\n",
    "            \n",
    "            scaled_in_train = self.scaler_x.fit_transform(\n",
    "                xs.drop(['Time [s]', 'id'], axis=1)\n",
    "            )\n",
    "            scaled_out_train = self.scaler_y.fit_transform(\n",
    "                ys.drop(['Time [s]', 'id'], axis=1)\n",
    "            )\n",
    "\n",
    "            with open(filepath, \"wb\") as fid:\n",
    "                pickle.dump([self.num_timesteps, self.scaler_x, self.scaler_y], fid)\n",
    "            #logging.info(\"Saved data preprocessing models to {filepath}\")\n",
    "            \n",
    "    def reshape_data(self, x_data, y_data):\n",
    "        \"\"\"\n",
    "        Reshape matrix data into sample shape for LSTM training.\n",
    "\n",
    "        Args:\n",
    "            x_data: DataFrame containing input features (columns) and time steps (rows).\n",
    "            y_data: Matrix containing output features (columns) and time steps (rows).\n",
    "            seq_length: Length of look back time steps for one time step of prediction.\n",
    "            num_timesteps: Number of time_steps per experiment.\n",
    "\n",
    "        Returns: Two np.ndarrays, the first of shape (samples, length of sequence,\n",
    "            number of features), containing the input data for the LSTM. The second\n",
    "            of shape (samples, number of output features) containing the expected output for each input\n",
    "            sample.\n",
    "        \"\"\"\n",
    "\n",
    "        x_data = torch.from_numpy(x_data.astype(np.float32))\n",
    "        y_data = torch.from_numpy(y_data.astype(np.float32))\n",
    "                \n",
    "        return x_data, y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the train, test, val splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(config[\"data\"][\"data_path\"], 'ML2019_*'))\n",
    "file_list = sorted(file_list, key = lambda x: int(x.split(\"Exp\")[1].strip(\".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "for x in file_list:\n",
    "    x = int(x.split(\"Exp\")[1].strip(\".csv\"))\n",
    "    if x >= config[\"data\"][\"min_exp\"] and x <= config[\"data\"][\"max_exp\"]:\n",
    "        experiments.append(x)\n",
    "\n",
    "# train_split, _test = train_test_split(experiments, test_size = 0.2)\n",
    "# valid_split, test_split = train_test_split(_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_split = train_split[:128]\n",
    "# valid_split = valid_split[:128]\n",
    "# test_split = test_split[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clustered/experiment_data/experiment_train_test_val_splits.pkl\", \"rb\") as fid:\n",
    "    train_split, valid_split, test_split = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_data = glob.glob(config[\"data\"][\"cached_dir\"] + \"/*pkl\")\n",
    "if len(cached_data):\n",
    "    fit = False\n",
    "else:\n",
    "    fit = True\n",
    "\n",
    "train_data_set = LoadGeckoData(\n",
    "    **config[\"data\"],\n",
    "    experiment_subset = train_split,\n",
    "    shuffle = True,\n",
    "    fit = fit\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_set.experiment_ids\n",
    "x, y = train_data_set.__getitem__(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1439, 35])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b86f2680150>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(y.shape[0]), y[:, 0])\n",
    "plt.plot(range(y.shape[0]), y[:, 1])\n",
    "plt.plot(range(y.shape[0]), y[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data_set = LoadGeckoData(\n",
    "    **config[\"data\"],\n",
    "    num_timesteps = train_data_set.num_timesteps,\n",
    "    experiment_subset = valid_split,\n",
    "    shuffle = False,\n",
    "    scaler_x = train_data_set.scaler_x,\n",
    "    scaler_y = train_data_set.scaler_y \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = LoadGeckoData(\n",
    "    **config[\"data\"],\n",
    "    num_timesteps = train_data_set.num_timesteps,\n",
    "    experiment_subset = test_split,\n",
    "    shuffle = False,\n",
    "    scaler_x = train_data_set.scaler_x,\n",
    "    scaler_y = train_data_set.scaler_y \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging.info(f\"Loading training data iterator using {config['iterator']['num_workers']} workers\")\n",
    "    \n",
    "train_dataloader = DataLoader(\n",
    "    train_data_set,\n",
    "    **config[\"iterator\"]\n",
    ")\n",
    "\n",
    "config[\"iterator\"][\"batch_size\"] = len(valid_split)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data_set,\n",
    "    **config[\"iterator\"]\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_set,\n",
    "    **config[\"iterator\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 output_size, \n",
    "                 hidden_dims = [100, 50], \n",
    "                 dropouts = [0.2, 0.2]):\n",
    "        \n",
    "        super(DenseNet, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        #self.embedding = nn.Embedding(train_data_set.num_timesteps, 16)\n",
    "        \n",
    "        self.model = [\n",
    "            nn.Linear(input_size, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(num_features=hidden_dims[0]),\n",
    "            nn.LeakyReLU()\n",
    "        ]\n",
    "        if len(hidden_dims) > 1:\n",
    "            if dropouts[0] > 0.0:\n",
    "                self.model.append(nn.Dropout(dropouts[0]))\n",
    "            for i in range(len(hidden_dims)-1):\n",
    "                self.model.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "                self.model.append(nn.BatchNorm1d(num_features=hidden_dims[i+1]))\n",
    "                self.model.append(nn.LeakyReLU())\n",
    "                if dropouts[i+1] > 0.0:\n",
    "                    self.model.append(nn.Dropout(dropouts[i+1]))\n",
    "        self.model.append(nn.Linear(hidden_dims[-1], output_size))\n",
    "        self.model.append(nn.Sigmoid())\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x1, x2 = x\n",
    "        #x1 = self.embedding(x1)\n",
    "        #x = torch.cat([x1, x2], 1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(**config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=35, out_features=10000, bias=True)\n",
       "    (1): BatchNorm1d(10000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=10000, out_features=29, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_cuda:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = LookaheadDiffGrad(model.parameters(),\n",
    "                              lr=config[\"optimizer\"][\"lr\"])\n",
    "                              #weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, yhat, y):\n",
    "        return torch.sqrt(self.mse(yhat,y))\n",
    "    \n",
    "class R2Score(nn.Module):\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        _num_examples = y.shape[1]\n",
    "        _sum_of_errors = torch.sum(torch.pow(y_pred - y, 2))\n",
    "        _y_sum = torch.sum(y)\n",
    "        _y_sq_sum = torch.sum(torch.pow(y, 2))\n",
    "        return _sum_of_errors / (_y_sq_sum - (_y_sum ** 2) / _num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Measure allocated memory after the call\n",
    "# torch.cuda.synchronize()\n",
    "# end_max_memory = torch.cuda.max_memory_allocated() / 1024**2\n",
    "# end_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "\n",
    "class BaseTrainer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 optimizer,\n",
    "                 train_gen, \n",
    "                 valid_gen, \n",
    "                 dataloader, \n",
    "                 valid_dataloader,\n",
    "                 start_epoch = 0,\n",
    "                 epochs = 100,\n",
    "                 window_size = 10,\n",
    "                 teacher_force = True,\n",
    "                 gamma = 0.5,\n",
    "                 device = \"cpu\",\n",
    "                 clip = 1.0,\n",
    "                 path_save = \"./\"):\n",
    "        \n",
    "        self.model = model\n",
    "        self.outsize = model.output_size\n",
    "        self.optimizer = optimizer\n",
    "        self.train_gen = train_gen\n",
    "        self.valid_gen = valid_gen\n",
    "        self.dataloader = dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.batch_size = dataloader.batch_size\n",
    "        self.path_save = path_save\n",
    "        self.device = device\n",
    "        \n",
    "        self.start_epoch = start_epoch \n",
    "        self.epochs = epochs\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.teacher_force = teacher_force\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        #self.criterion = nn.MSELoss()\n",
    "        \n",
    "        timesteps = self.train_gen.num_timesteps\n",
    "        self.time_range = list(range(timesteps))\n",
    "                \n",
    "        # Gradient clipping through hook registration\n",
    "        for p in self.model.parameters():\n",
    "            p.register_hook(lambda grad: torch.clamp(grad, -clip, clip))\n",
    "        logger.info(f\"Clipping gradients to range [-{clip}, {clip}]\")\n",
    "        \n",
    "        # Create the save directory if it does not exist\n",
    "        try:\n",
    "            os.makedirs(path_save)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def criterion(self, y_true, y_pred):\n",
    "        \n",
    "        y_true_precursor = y_true[:, :, 0]\n",
    "        y_pred_precursor = y_pred[:, :, 0]\n",
    "        \n",
    "        y_true_gas = y_true[:, :, 1:15]\n",
    "        y_pred_gas = y_pred[:, :, 1:15]\n",
    "        \n",
    "        y_true_aero = y_true[:, :, 15:]\n",
    "        y_pred_aero = y_pred[:, :, 15:]\n",
    "        \n",
    "        mse_precursor = nn.MSELoss()(y_true_precursor, y_pred_precursor)\n",
    "        mse_gas = nn.MSELoss()(y_true_gas, y_pred_gas)\n",
    "        mse_aero = nn.MSELoss()(y_true_aero, y_pred_aero)\n",
    "        mse = mse_precursor + mse_gas + mse_aero\n",
    "        \n",
    "        kld_gas = nn.KLDivLoss()(\n",
    "            F.log_softmax(y_pred_gas),\n",
    "            F.softmax(y_true_gas)\n",
    "        )\n",
    "        kld_aero = nn.KLDivLoss()(\n",
    "            F.log_softmax(y_pred_aero),\n",
    "            F.softmax(y_true_aero)\n",
    "        )\n",
    "        return mse + (kld_gas + kld_aero)\n",
    "        \n",
    "    def train_one_epoch(self, epoch, steps = 1e10):\n",
    "        \n",
    "        self.model.train()\n",
    "        batches_per_epoch = int(np.ceil(self.train_gen.__len__() / self.batch_size))\n",
    "        batch_group_generator = tqdm(\n",
    "            self.dataloader,\n",
    "            total=batches_per_epoch, \n",
    "            leave=True\n",
    "        )\n",
    "        \n",
    "#         if epoch == 0:\n",
    "#             self.idx = 0\n",
    "#         else:\n",
    "#             self.idx += 1 \n",
    "#         self.window = self.time_range[self.idx * self.window_size : (self.idx + 1) * self.window_size]\n",
    "        \n",
    "#         if len(self.window) == 0:\n",
    "#             self.idx = 0\n",
    "#             self.window = self.time_range[self.idx * self.window_size : (self.idx + 1) * self.window_size]\n",
    "            \n",
    "        #self.window = self.time_range[0:(self.idx * self.window_size)]\n",
    "        self.window = self.time_range #[:self.window_size]\n",
    "    \n",
    "        epoch_losses = {\"loss\": []}\n",
    "        for batch_idx, (x, y) in enumerate(batch_group_generator):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "#             self.window = random.choice(self.time_range)\n",
    "#             if self.window_size > 0:\n",
    "#                 self.window = [self.window + (i+1) for i in range(self.window_size)]\n",
    "#             else:\n",
    "#                 self.window = [self.window]\n",
    "#             self.window = [x for x in self.window if x <= max(self.time_range)]\n",
    "            #window = random.sample(self.time_range, self.window_size) # only works when forcing = 1.0\n",
    "                        \n",
    "            y_true, y_pred, weights = [], [], []\n",
    "            for i in range(y.size(1)):\n",
    "                next_x = self.model(x[:,i,:])\n",
    "                y_true.append(y[:, i])\n",
    "                y_pred.append(next_x)                                \n",
    "                if i < (y.size(1)-1):\n",
    "                    if (epoch == self.epochs - 1) or (not self.teacher_force):\n",
    "                        x = x.clone()\n",
    "                        x[:, i+1, :self.outsize] = next_x                        \n",
    "                    else:\n",
    "                        cost = self.tf_annealer(epoch) \n",
    "                        idx = [bn for bn in range(x.size(0)) if cost < random.random()]\n",
    "                        if len(idx) > 0:\n",
    "                            x = x.clone() # next line is in-place op, messes up grad. comp. \n",
    "                            x[idx, i+1, :self.outsize] = next_x[idx]  \n",
    "                            \n",
    "            y_true = torch.stack(y_true).permute(1,0,2)\n",
    "            y_pred = torch.stack(y_pred).permute(1,0,2)\n",
    "            loss = self.criterion(y_true, y_pred)   \n",
    "            epoch_losses[\"loss\"].append(loss.item())\n",
    "\n",
    "            # backprop after experiment\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # update tqdm\n",
    "            to_print = \"loss: {:.3f}\".format(np.mean(epoch_losses[\"loss\"]))\n",
    "            batch_group_generator.set_description(to_print)\n",
    "            batch_group_generator.update()\n",
    "            \n",
    "#             if batch_idx % steps == 0:\n",
    "#                 break\n",
    "            \n",
    "        return np.mean(epoch_losses[\"loss\"])\n",
    "            \n",
    "    def test(self, epoch):\n",
    "\n",
    "        self.model.eval()\n",
    "        batches_per_epoch = int(np.ceil(self.valid_gen.__len__() / self.batch_size))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_group_generator = tqdm(\n",
    "                self.valid_dataloader,\n",
    "                total=batches_per_epoch, \n",
    "                leave=True\n",
    "            )\n",
    "            \n",
    "            epoch_losses = {\"loss\": []}\n",
    "            for (x, y) in batch_group_generator:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                y_true, y_pred = [], []\n",
    "                for i in range(y.size(1)):\n",
    "                    next_x = self.model(x[:,i,:])\n",
    "                    y_true.append(y[:, i])\n",
    "                    y_pred.append(next_x)\n",
    "                    if i < (y.shape[1]-1):\n",
    "                        x[:, i+1, :self.outsize] = next_x # never \"force\" on eval\n",
    "                y_true = torch.stack(y_true).permute(1,0,2)\n",
    "                y_pred = torch.stack(y_pred).permute(1,0,2)\n",
    "                loss = self.criterion(y_true, y_pred)\n",
    "                epoch_losses[\"loss\"].append(loss.item())\n",
    "\n",
    "                # update tqdm\n",
    "                to_print = \"val_loss: {:.3f}\".format(np.mean(epoch_losses[\"loss\"]))\n",
    "                batch_group_generator.set_description(to_print)\n",
    "                batch_group_generator.update()\n",
    "            \n",
    "        return np.mean(epoch_losses[\"loss\"]) \n",
    "    \n",
    "    \n",
    "    def train(self,\n",
    "              scheduler,\n",
    "              early_stopping,\n",
    "              metrics_logger):\n",
    "        \n",
    "        logger.info(\n",
    "            f\"Training the model for up to {self.epochs} epochs starting at epoch {self.start_epoch}\"\n",
    "        )\n",
    "        \n",
    "        flag = isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "        \n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "            train_loss = self.train_one_epoch(epoch)\n",
    "            test_loss = self.test(epoch)\n",
    "\n",
    "            scheduler.step(test_loss if flag else epoch)\n",
    "            early_stopping(epoch, test_loss, self.model, self.optimizer)\n",
    "\n",
    "            # Write results to the callback logger \n",
    "            result = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": test_loss,\n",
    "                \"lr\": early_stopping.print_learning_rate(self.optimizer),\n",
    "                \"teacher_forcing_score\": self.tf_annealer(epoch) if self.teacher_force else 1.0\n",
    "            }\n",
    "            metrics_logger.update(result)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                logger.info(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    def tf_annealer(self, epoch):\n",
    "        return 1.0 * self.gamma ** epoch # 1/(1 + self.decay * epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BaseTrainer(\n",
    "    model, \n",
    "    optimizer,\n",
    "    train_data_set, \n",
    "    valid_data_set, \n",
    "    train_dataloader, \n",
    "    valid_dataloader,\n",
    "    device = device,\n",
    "    **config[\"trainer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LR annealing scheduler \n",
    "if \"ReduceLROnPlateau\" in config[\"callbacks\"]:\n",
    "    schedule_config = config[\"callbacks\"][\"ReduceLROnPlateau\"]\n",
    "    scheduler = ReduceLROnPlateau(trainer.optimizer, **schedule_config)\n",
    "    #logging.info(\n",
    "    #    f\"Loaded ReduceLROnPlateau learning rate annealer with patience {schedule_config['patience']}\"\n",
    "    #)\n",
    "elif \"ExponentialLR\" in config[\"callbacks\"]:\n",
    "    schedule_config = config[\"callbacks\"][\"ExponentialLR\"]\n",
    "    scheduler = ExponentialLR(trainer.optimizer, **schedule_config)\n",
    "    #logging.info(\n",
    "    #    f\"Loaded ExponentialLR learning rate annealer with reduce factor {schedule_config['gamma']}\"\n",
    "    #)\n",
    "\n",
    "# Early stopping\n",
    "checkpoint_config = config[\"callbacks\"][\"EarlyStopping\"]\n",
    "early_stopping = EarlyStopping(**checkpoint_config)\n",
    "\n",
    "# Write metrics to csv each epoch\n",
    "metrics_logger = MetricsLogger(**config[\"callbacks\"][\"MetricsLogger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.188: 100%|| 800/800 [08:29<00:00,  1.57it/s]\n",
      "val_loss: 0.930:   1%|          | 1/101 [00:05<08:26,  5.06s/it]\n",
      "loss: 0.175: 100%|| 800/800 [08:24<00:00,  1.59it/s]\n",
      "val_loss: 0.972:   1%|          | 1/101 [00:05<08:35,  5.15s/it]\n",
      "loss: 0.176: 100%|| 800/800 [12:10<00:00,  1.09it/s]\n",
      "val_loss: 0.986:   1%|          | 1/101 [00:06<10:17,  6.18s/it]\n",
      "loss: 0.175: 100%|| 800/800 [09:35<00:00,  1.39it/s]\n",
      "val_loss: 0.984:   1%|          | 1/101 [00:05<09:36,  5.76s/it]\n",
      "loss: 0.175: 100%|| 800/800 [09:10<00:00,  1.45it/s]\n",
      "val_loss: 0.917:   1%|          | 1/101 [00:08<13:46,  8.27s/it]\n",
      "loss: 0.174: 100%|| 800/800 [10:12<00:00,  1.31it/s]\n",
      "val_loss: 0.945:   1%|          | 1/101 [00:05<09:00,  5.40s/it]\n",
      "loss: 0.177: 100%|| 800/800 [09:48<00:00,  1.36it/s]\n",
      "val_loss: 0.964:   1%|          | 1/101 [00:05<08:49,  5.30s/it]\n",
      "loss: 0.172: 100%|| 800/800 [09:31<00:00,  1.40it/s]\n",
      "val_loss: 0.936:   1%|          | 1/101 [00:05<08:38,  5.18s/it]\n",
      "loss: 0.173:  58%|    | 463/800 [05:37<03:58,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train(scheduler, early_stopping, metrics_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"clustered/config.yml\") as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(**config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load(\"results/9_23/pandas/best.pt\", \n",
    "                        map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_dict[\"model_state_dict\"])\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if is_cuda:\n",
    "#     model = model.to(device)\n",
    "device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, y) in test_dataloader:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "preds = []\n",
    "for t in tqdm(range(y.shape[1])):\n",
    "    next_x = model(x[:,t,:])\n",
    "    preds.append(next_x)\n",
    "    #loss.append(RMSELoss()(next_x, y[:,t]))\n",
    "    \n",
    "    #if t < (y.shape[1]-1):\n",
    "    #    x[:, t+1, :model.output_size] = next_x # never force on test\n",
    "\n",
    "#loss = torch.mean(torch.stack(loss))\n",
    "preds = torch.stack(preds, axis = -1).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsteps = y.shape[1]\n",
    "\n",
    "plt.plot(range(nsteps), y[1, :nsteps, 0])\n",
    "plt.plot(range(nsteps), y[1, :nsteps, 1:15].sum(-1)/14)\n",
    "plt.plot(range(nsteps), y[1, :nsteps, 15:].sum(-1)/14)\n",
    "\n",
    "plt.ylabel(\"Precursor/Gas/Aerosol\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "\n",
    "plt.ylim([0.0, 1.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(nsteps), preds[1, :nsteps, 0].detach().numpy())\n",
    "plt.plot(range(nsteps), preds[1, :nsteps, 1:15].detach().numpy().sum(-1)/14)\n",
    "plt.plot(range(nsteps), preds[1, :nsteps, 15:].detach().numpy().sum(-1)/14)\n",
    "\n",
    "plt.ylabel(\"Precursor/Gas/Aerosol\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "\n",
    "plt.ylim([0.0, 1.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.MSELoss()(y[:,:nsteps,:], preds[:, :nsteps, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.KLDivLoss()(\n",
    "    F.log_softmax(preds[:, :nsteps, 1:15]), \n",
    "    F.softmax(y[:, :nsteps, 1:15])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.KLDivLoss()(\n",
    "    F.log_softmax(preds[:, :nsteps, 15:]), \n",
    "    F.softmax(y[:, :nsteps, 15:])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
